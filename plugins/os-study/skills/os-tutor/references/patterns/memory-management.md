# Memory Management Pattern Guide

Quick reference for memory management concepts, paging, segmentation, and address translation.

## Memory Organization Comparison

| Approach | Fragmentation | Flexibility | HW Support | Complexity | Protection |
|----------|---------------|-------------|------------|------------|------------|
| **Contiguous Allocation** | External (high) | Low | Minimal | Low | Base/limit registers |
| **Segmentation** | External (medium) | High (logical units) | Segment table | Medium | Per-segment permissions |
| **Paging** | Internal (small) | High | Page table + TLB | Medium | Per-page permissions |
| **Segmentation + Paging** | Internal only | Very High | Both | **High** | Both granularities |

## Key Concepts

### 1. Address Binding

**Three Times for Binding**:

| When | Type | Example | Relocatable? |
|------|------|---------|--------------|
| **Compile Time** | Absolute code | MS-DOS .COM, embedded systems | No (must load at fixed address) |
| **Load Time** | Relocatable code | Linked with base address at load | No (after loading) |
| **Execution Time** | Dynamic relocation | Modern OSes with MMU | **Yes** (can move during execution) |

**Execution-time binding requires hardware support** (MMU with base/limit registers)

### 2. Logical vs Physical Address

| Address Type | Visible To | Generated By | Also Called |
|--------------|------------|--------------|-------------|
| **Logical (Virtual)** | CPU/Program | Compiler, CPU | Virtual address |
| **Physical** | Memory hardware | MMU | Real address |

**Memory Management Unit (MMU)**: Hardware that translates logical → physical at runtime

**Simple MMU** (Base + Limit):
```
Physical Address = Logical Address + Base Register
if (Logical Address >= Limit) → TRAP (segment fault)
```

### 3. Fragmentation Types

**External Fragmentation**:
- **Definition**: Free memory exists but is non-contiguous (scattered holes)
- **Problem**: Can't satisfy request even though total free space sufficient
- **Occurs in**: Contiguous allocation, segmentation
- **Solution**: Compaction (expensive), paging

**Internal Fragmentation**:
- **Definition**: Allocated memory larger than requested, waste inside allocated block
- **Problem**: Small waste per allocation, adds up system-wide
- **Occurs in**: Paging (last page of process), fixed-size partitions
- **Solution**: Smaller page size (but more overhead)

**Example**:
- Process needs 72KB
- Page size = 4KB
- Pages needed = ⌈72KB / 4KB⌉ = 18 pages
- **Internal fragmentation** = (18 * 4KB) - 72KB = 0KB (lucky, exact fit!)
- If process needs 73KB → 19 pages → 76KB allocated → **3KB wasted**

## Paging Fundamentals

**Core Idea**: Divide logical memory into fixed-size blocks (**pages**), physical memory into same-size blocks (**frames**)

**Key Components**:
- **Page**: Fixed-size block of logical memory (typically 4KB - 4MB)
- **Frame**: Fixed-size block of physical memory (same size as page)
- **Page Table**: Maps page numbers to frame numbers
- **Page Number (p)**: High-order bits of logical address
- **Page Offset (d)**: Low-order bits (position within page)

### Address Translation Formula

**Given**: Logical address, page size

**Steps**:
1. **Split address**:
   - Page number (p) = Logical Address / Page Size
   - Page offset (d) = Logical Address % Page Size
2. **Look up frame number** (f) in page table using p
3. **Compute physical address** = (f × Page Size) + d

**Example**: Translate logical address 2050 with page size 1024 bytes

1. Page number p = 2050 / 1024 = 2
2. Offset d = 2050 % 1024 = 2
3. Page table: page 2 → frame 7
4. Physical address = (7 × 1024) + 2 = **7170**

### Page Table Structure

**Simplest**: Array indexed by page number

| Page # | Frame # | Valid Bit | Protection Bits |
|--------|---------|-----------|-----------------|
| 0 | 4 | 1 | RW |
| 1 | 7 | 1 | R |
| 2 | 2 | 1 | RWX |
| 3 | - | 0 | - |

**Problem**: Large address spaces need huge page tables!
- 32-bit address, 4KB pages → 2^20 entries per process
- 64-bit address, 4KB pages → 2^52 entries (impossible!)

**Solutions**: Multilevel paging, hashed page tables, inverted page tables

## Translation Lookaside Buffer (TLB)

**Problem**: Page table in memory → every memory access requires TWO memory accesses (page table + actual data)

**Solution**: TLB = fast cache of recent page table entries

**Structure**:
| Page # | Frame # | Valid | Protection |
|--------|---------|-------|------------|
| 2 | 7 | 1 | RW |
| 15 | 23 | 1 | R |
| ... | ... | ... | ... |

**Operation**:
1. Check TLB for page number
2. **TLB Hit**: Use frame number directly (fast!)
3. **TLB Miss**: Access page table in memory, update TLB

### Effective Access Time (EAT) Calculation

**Formula**: EAT = (TLB Hit Time × Hit Ratio) + (TLB Miss Time × (1 - Hit Ratio))

Where:
- **TLB Miss Time** = TLB access + Page table access + Memory access

**Example**:
- Memory access time = 100ns
- TLB access time = 20ns
- TLB hit ratio = 80%

**Calculations**:
- TLB hit time = 20ns + 100ns = 120ns (TLB + memory)
- TLB miss time = 20ns + 100ns + 100ns = 220ns (TLB + page table + memory)
- EAT = (0.80 × 120ns) + (0.20 × 220ns) = 96ns + 44ns = **140ns**

**Slowdown**: 140ns vs 100ns direct = **40% overhead**

**With 98% hit ratio**:
- EAT = (0.98 × 120ns) + (0.02 × 220ns) = 117.6ns + 4.4ns = **122ns**
- Slowdown: Only **22% overhead**

**Key Insight**: TLB hit ratio critical for performance!

## Hierarchical (Multilevel) Page Tables

**Problem**: Single-level page table too large for large address spaces

**Solution**: Page the page table itself!

**Two-Level Example** (32-bit address, 4KB pages):

**Logical Address Structure**:
```
| Page Directory (10 bits) | Page Table (10 bits) | Offset (12 bits) |
```

**Translation Steps**:
1. Use outer page number (p1) to index page directory → get page table location
2. Use inner page number (p2) to index page table → get frame number
3. Use offset (d) within frame

**Advantages**:
- Only need page directory + active page tables in memory
- Unused address spaces don't need page tables
- Typical savings: 90%+ for sparse address spaces

**Disadvantages**:
- More memory accesses per translation (solved by TLB)
- More complex

**Example**: x86-64 uses **four-level paging**!

## Segmentation

**Concept**: Divide program into logical units (segments)

**Segments**: code, data, stack, heap, symbol table, etc.

**Advantages over Paging**:
- Matches programmer's view of memory
- Easy to share (e.g., shared library = shared code segment)
- Different protection per segment (execute for code, read-write for data)

**Segmentation Table**:
| Segment # | Base Address | Limit | Protection |
|-----------|--------------|-------|------------|
| 0 (code) | 1400 | 1000 | RX |
| 1 (data) | 6300 | 400 | RW |
| 2 (stack) | 4300 | 1100 | RW |

**Address Translation**:
```
Logical Address = <segment number, offset>
if (offset > limit) → TRAP (segmentation fault)
Physical Address = base + offset
```

**Problem**: Variable-size segments → external fragmentation

## Segmentation + Paging (Intel IA-32)

**Combines both**:
1. Logical address → Segment table → Linear address
2. Linear address → Page table → Physical address

**Advantage**: Get benefits of both (logical units + no external fragmentation)
**Disadvantage**: Complex, double translation

## Page Size Selection

**Trade-offs**:

| Aspect | Small Pages (4KB) | Large Pages (2MB - 1GB) |
|--------|-------------------|-------------------------|
| **Internal fragmentation** | Low (avg 2KB waste) | High (avg 0.5-512MB waste) |
| **Page table size** | Large (many entries) | Small (few entries) |
| **TLB coverage** | Low | **High** (each entry covers more memory) |
| **Page fault overhead** | High (relative to transfer) | Low |
| **Disk I/O efficiency** | Poor (small transfers) | **Good** (large transfers) |
| **Locality match** | Better for small working sets | Better for large working sets |

**Modern Trend**: Support multiple page sizes
- Base = 4KB for general use
- Huge pages (2MB, 1GB) for databases, VMs

**TLB Reach** = (TLB Size) × (Page Size)
- Example: 128-entry TLB, 4KB pages → 512KB reach
- With 2MB pages → 256MB reach (500× better!)

## Common Exam Questions

### "Translate logical address 0x3A7C with 4KB pages"

**Given**: 4KB pages = 2^12 bytes = 4096 bytes

**Step 1**: Extract page number and offset
- Address 0x3A7C = 14972 (decimal)
- Page number = 14972 / 4096 = **3**
- Offset = 14972 % 4096 = **2684** (or 0xA7C in hex)

**Step 2**: Look up page 3 in page table
- Assume page table shows: Page 3 → Frame 6

**Step 3**: Calculate physical address
- Physical address = (Frame × Page Size) + Offset
- = (6 × 4096) + 2684
- = 24576 + 2684
- = **27260** (decimal) or **0x6A7C** (hex)

**Note**: Offset bits (low 12 bits) unchanged in translation!

### "Calculate effective access time with TLB hit ratio"

**Given**:
- Memory access time = 10ns
- TLB access time = 1ns
- TLB hit ratio = 95%

**Solution**:
- **TLB Hit**: 1ns (TLB) + 10ns (memory) = 11ns
- **TLB Miss**: 1ns (TLB) + 10ns (page table) + 10ns (memory) = 21ns
- **EAT** = (0.95 × 11ns) + (0.05 × 21ns) = 10.45ns + 1.05ns = **11.5ns**

**Comparison**: Without TLB = 10ns (page table) + 10ns (memory) = 20ns
**Improvement**: 11.5ns vs 20ns = **42.5% faster**

**What if hit ratio = 99%?**
- EAT = (0.99 × 11ns) + (0.01 × 21ns) = 10.89ns + 0.21ns = **11.1ns**
- Nearly eliminates paging overhead!

### "Why use multilevel page tables?"

**Single-Level Problem** (32-bit address space, 4KB pages):
- Page table entries = 2^32 bytes / 2^12 bytes = 2^20 = **1,048,576 entries**
- If each PTE = 4 bytes → page table size = 4MB **per process**
- With 100 processes → 400MB just for page tables!
- Most of address space unused → wasting memory

**Two-Level Solution**:
- Outer table: 1024 entries × 4 bytes = 4KB
- Inner tables: Only create for used regions
- Typical process uses < 10% of address space → 90% savings
- Example: Only need outer table + 100 inner tables = 4KB + 400KB = 404KB vs 4MB

**Extreme**: 64-bit addresses
- Single-level would need 2^52 entries (with 4KB pages) = **impossible**!
- Four-level paging makes it feasible

### "Compare internal vs external fragmentation"

**Scenario 1: Contiguous Allocation**

| Process | Size | Allocated |
|---------|------|-----------|
| P1 | 100KB | 100KB block |
| P2 | 150KB | 150KB block |
| P3 | 200KB | 200KB block |

After P2 terminates:
```
[P1: 100KB] [HOLE: 150KB] [P3: 200KB]
```

New process P4 needs 200KB → **Can't fit!** (even though 150KB free)
- **External fragmentation** = 150KB unusable

**Solution**: Compact: move P3 down to fill hole (expensive!)

**Scenario 2: Paging**

| Process | Size | Pages Needed (4KB pages) |
|---------|------|--------------------------|
| P1 | 10KB | 3 pages = 12KB |
| P2 | 15KB | 4 pages = 16KB |

- P1 **Internal fragmentation** = 12KB - 10KB = 2KB
- P2 **Internal fragmentation** = 16KB - 15KB = 1KB
- Total waste = 3KB

**External fragmentation** = 0 (pages can go in any frames)

**Trade-off**:
- Paging eliminates external fragmentation
- But introduces internal fragmentation
- Internal fragmentation usually small (avg = page size / 2)

### "How does shared memory work with paging?"

**Concept**: Multiple processes map same physical frames into their address spaces

**Example**: Shared library (e.g., libc)

**Process A Page Table**:
| Page | Frame |
|------|-------|
| 0 | 10 |
| 1 | **50** ← Shared |
| 2 | **51** ← Shared |
| 3 | 12 |

**Process B Page Table**:
| Page | Frame |
|------|-------|
| 0 | 20 |
| 1 | **50** ← Shared |
| 2 | **51** ← Shared |
| 3 | 22 |

Both processes' pages 1-2 map to same frames 50-51 (the shared library code)

**Advantages**:
1. **Save memory**: One copy serves multiple processes
2. **Inter-process communication**: Shared data region
3. **Consistency**: Updates visible to all processes

**Protection**: Typically read-only for shared code, read-write for shared data

## Advanced Concepts

### Inverted Page Tables

**Problem**: Page table size grows with virtual address space (bad for 64-bit)

**Solution**: Page table size based on **physical memory** instead

**Structure**: One entry per **frame** (not per page)
| Frame # | Process ID | Page # | Protection |
|---------|------------|--------|------------|
| 0 | P1 | 5 | RW |
| 1 | P2 | 10 | R |
| 2 | P1 | 3 | RWX |

**Lookup**: Hash (PID, page#) to find frame
- **Advantage**: Small table (size = # frames, not # pages)
- **Disadvantage**: Slower lookup (hash search vs array index)
- **Used by**: IBM PowerPC, Oracle SPARC

### ARM Address Translation

**3 or 4 levels** depending on configuration
- **Level 0 (optional)**: For 48-bit addresses
- **Level 1**: Table descriptor → Level 2 table
- **Level 2**: Either:
  - Table descriptor → Level 3 table
  - Block descriptor → 2MB page (huge page)
- **Level 3**: 4KB page descriptor → Frame

**TLB**: Separate instruction and data TLBs, multiple levels

### Memory Protection Bits

**Typical Protection Bits**:
- **Valid/Invalid**: Is page in physical memory?
- **Read (R)**: Can read from page?
- **Write (W)**: Can write to page?
- **Execute (X)**: Can execute code from page?
- **User/Kernel**: Who can access?

**Example Use Cases**:
- **Code segment**: R + X (readable, executable, not writable)
- **Data segment**: R + W (readable, writable, not executable)
- **Stack**: R + W (data, not code)
- **Read-only data**: R only

**Violation** → Protection fault → OS handles (usually terminates process)

## Decision Guide

**Use Contiguous Allocation when**:
- Embedded systems with fixed memory
- Simple requirements
- No fragmentation issues expected

**Use Segmentation when**:
- Logical units important (sharing, protection)
- Can tolerate external fragmentation
- Variable-size segments natural for application

**Use Paging when**:
- General-purpose OS
- Need to eliminate external fragmentation
- Large address space support
- Hardware has TLB support

**Use Segmentation + Paging when**:
- Want logical segments AND no external fragmentation
- Can afford complexity
- Hardware supports both (e.g., x86)

**Use Multilevel Paging when**:
- Large (>32-bit) address spaces
- Sparse memory usage (not all pages used)
- Can afford extra memory accesses (TLB mitigates)

**Use Huge Pages when**:
- Large working sets (databases, VMs)
- Want to improve TLB reach
- Acceptable internal fragmentation trade-off

## Performance Tips

### Optimizing TLB Hit Rate
1. **Increase TLB coverage**: Use larger pages
2. **Improve locality**: Access memory sequentially when possible
3. **Reduce working set**: Keep frequently accessed data together

### Minimizing Page Table Memory
1. Use multilevel paging (only allocate what's needed)
2. Use inverted page tables (for very large address spaces)
3. Share pages between processes where possible

### Address Translation Speed
1. Maximize TLB hit ratio (critical!)
2. Use hardware page-walk support
3. Cache page table entries in CPU
4. Use huge pages to reduce table depth
